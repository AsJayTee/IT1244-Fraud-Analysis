{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Importing functions\n",
    "\n",
    "def import_client(filepath : str = \"Data/client.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to import client dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def import_invoice(filepath : str = \"Data/invoice.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to import invoice dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing functions\n",
    "\n",
    "def convert_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts date column to an integer representing \n",
    "    the number of days since the earliest date.\n",
    "    *Column name 'date' is fixed for both datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], dayfirst = True)\n",
    "    \n",
    "    # Find the earliest date\n",
    "    earliest_date = df['date'].min()\n",
    "    \n",
    "    # Calculate the number of days since the earliest date\n",
    "    df['date'] = (df['date'] - earliest_date).dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_duplicates(df : pd.DataFrame, logs : bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prints the result of a duplicate check.\n",
    "    Drops duplicates if they exist.\n",
    "    \"\"\"\n",
    "    if df.duplicated().any(): # Duplicates check\n",
    "        if logs:\n",
    "            print(\"Duplicates found! Cleaning them up...\")\n",
    "        df = df.drop_duplicates() # Drops duplicates\n",
    "        df = df.reset_index(drop = True) # Resets indexes\n",
    "    elif logs:\n",
    "        print(\"No duplicates found!\")\n",
    "    return df\n",
    "\n",
    "def convert_to_categorical(\n",
    "        df : pd.DataFrame,\n",
    "        cols : list[str]\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts list of column names to categorical datatype.\n",
    "    \"\"\"\n",
    "    df[cols] = df[cols].astype('category')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Feature Engineering functions\n",
    "\n",
    "def aggregate_invoice(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate the invoice dataframe by id and generate features.\n",
    "    Calculates sum, mean, max, and std for each consommation_level and date\n",
    "    and counts number of invoices under each id.\n",
    "    \"\"\"\n",
    "    df = df.groupby('id').agg({ # Aggregate by id\n",
    "        # Calculate sum, mean, max, and std for each consm_level and date\n",
    "        'consommation_level_1': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_2': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_3': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_4': ['sum', 'mean', 'max', 'std'],\n",
    "        'date': ['sum', 'mean', 'max', 'std'],\n",
    "        'counter_statue': 'count', # Count number of invoices\n",
    "    }).reset_index()\n",
    "    df.columns = [\n",
    "        # Join by _ if more than 2 parts to the column name exists\n",
    "        '_'.join(col).strip() if col[1] \n",
    "        else col[0] # Else use original name\n",
    "        for col in df.columns.values # For each column name value\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def manual_fix_names(\n",
    "        df : pd.DataFrame,\n",
    "        new_col_names : list[str]\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Manually sets the column names of a dataframe.\n",
    "    \"\"\"\n",
    "    df.columns = new_col_names\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Joining functions\n",
    "\n",
    "def merge(\n",
    "        client_df : pd.DataFrame,\n",
    "        invoice_df : pd.DataFrame,\n",
    "        merge_by : str = \"id\"\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges two dataframes.\n",
    "    Merges on 'id' column by default (for client and invoice).\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(\n",
    "        client_df, invoice_df, on = merge_by)\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensionality Reduction functions\n",
    "\n",
    "def prep_dataframe(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [],\n",
    "        scale : bool = True,\n",
    "        OHE : bool = True\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For scaling and one-hot encoding dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        scale (bool): Indicate if data should be scaled\n",
    "        OHE (bool): Indicate if categorical variables should undergo One-Hot Encoding\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns scaled/One-Hot Encoded\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.drop(columns = ['id']) \n",
    "    # Drop id column as it is not useful for predicting\n",
    "\n",
    "    cat_col_names.remove('id') \n",
    "    # Remove 'id' as it no longer exists in dataframe\n",
    "\n",
    "    if response_col_name in cat_col_names:\n",
    "        cat_col_names.remove(response_col_name)\n",
    "    # Exclude doing OHE on response column\n",
    "\n",
    "    # Identify numerical columns \n",
    "    # by excluding categorical and response columns\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in cat_col_names \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    if scale:\n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "    if OHE:\n",
    "        # One-hot encode categorical features\n",
    "        encoder = OneHotEncoder(sparse_output = False, drop = 'first')  \n",
    "        # drop = 'first' to avoid dummy variable trap\n",
    "        X_cat = encoder.fit_transform(X_cat)\n",
    "    \n",
    "    # Combine the scaled numerical and encoded categorical features\n",
    "    X_prep = pd.DataFrame(\n",
    "        data = np.hstack((X_num, X_cat)),  # Horizontal stack to combine arrays\n",
    "        columns = num_col_names + list(encoder.get_feature_names_out(cat_col_names))\n",
    "    )\n",
    "\n",
    "    # Create a new DataFrame including the response variable\n",
    "    prep_df = pd.DataFrame(X_prep, columns = X_prep.columns)\n",
    "    prep_df[response_col_name] = y\n",
    "\n",
    "    return prep_df\n",
    "\n",
    "def principal_component_analysis(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [], \n",
    "        var : float = 0.95, \n",
    "        logs : bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For reducing the dimensions of a dataframe using PCA.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        var (float): Proportion of variance that should be preserved\n",
    "        logs (bool): Indicate if logs should be printed\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame that has undergone PCA\n",
    "    \"\"\"\n",
    "\n",
    "    if response_col_name in cat_col_names:\n",
    "        cat_col_names.remove(response_col_name)\n",
    "    # Exclude doing OHE on response column\n",
    "\n",
    "    # Identify numerical columns \n",
    "    # by excluding categorical and response columns\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in cat_col_names \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    pca = PCA(n_components = var) \n",
    "    # Keep 'var' proportion of the variance : default 95%\n",
    "    X_pca = pca.fit_transform(X_num) \n",
    "\n",
    "    pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "    df_pca = pd.DataFrame(X_pca, columns = pca_columns)\n",
    "\n",
    "    df_modified = pd.concat(\n",
    "        [\n",
    "            df_pca, \n",
    "            pd.DataFrame(X_cat, columns = cat_col_names), \n",
    "            pd.Series(y, name = response_col_name)\n",
    "        ], \n",
    "        axis = 1)\n",
    "    \n",
    "    if logs:\n",
    "        print(f\"Number of components selected: {pca.n_components_}\")\n",
    "        print(\"Explained variance ratio for each component:\", pca.explained_variance_ratio_)\n",
    "        print(\"Cumulative explained variance:\", pca.explained_variance_ratio_.cumsum())\n",
    "        print(\"Final DataFrame with PCA applied to numeric columns:\")\n",
    "        print(df_modified.head())\n",
    "\n",
    "    return df_modified\n",
    "\n",
    "def filter_low_variance(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        threshold: float = 0.1\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter numeric columns based on a variance threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        threshold (float): The variance threshold for filtering columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with low variance numeric columns removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify numerical columns \n",
    "    # by excluding response column\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col != response_col_name\n",
    "    ]\n",
    "\n",
    "    # Calculate the variance for each numeric column\n",
    "    variances = df[num_col_names].var()\n",
    "\n",
    "    # Keep response column\n",
    "    high_variance_cols = [response_col_name]\n",
    "\n",
    "    # Filter columns with variance greater than the threshold : default 0.1\n",
    "    cols_to_keep = variances[variances > threshold].index.tolist()\n",
    "    high_variance_cols.extend(cols_to_keep)\n",
    "\n",
    "    # Create a new DataFrame with only the selected columns\n",
    "    filtered_df = df[high_variance_cols]\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualisation functions\n",
    "\n",
    "def heatmap(\n",
    "        df : pd.DataFrame,\n",
    "        excluded_cols : list[str],\n",
    "        annotate : bool = False\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Plots a Correlation Heatmap.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        excluded_cols (list[str]): Columns to be excluded from the heatmap.\n",
    "        annotate (float): To annotate the heatmap with correlation values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify desired columns \n",
    "    # by excluding undesired column\n",
    "    col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in excluded_cols\n",
    "    ]\n",
    "\n",
    "    X = df[col_names]\n",
    "\n",
    "    corr_matrix = X.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        corr_matrix, annot = annotate, cmap = 'coolwarm', fmt = '.2f'\n",
    "    )\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Balancing functions\n",
    "\n",
    "def undersample_data(\n",
    "        df: pd.DataFrame, \n",
    "        response_col_name: str, \n",
    "        proportion: float = 1\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform undersampling to balance the classes in the dataset based on a specified proportion.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        proportion (float): The desired ratio of minority to majority class samples (e.g., 0.5 for 1:2).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A balanced DataFrame with samples from both classes according to the specified proportion.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the majority and minority classes\n",
    "    majority_class = df[response_col_name].value_counts().idxmax()\n",
    "    minority_class = df[response_col_name].value_counts().idxmin()\n",
    "\n",
    "    # Split the DataFrame into majority and minority\n",
    "    df_majority = df[df[response_col_name] == majority_class]\n",
    "    df_minority = df[df[response_col_name] == minority_class]\n",
    "\n",
    "    # Calculate the number of samples to take from the majority class\n",
    "    n_minority = len(df_minority)\n",
    "    n_majority = int(n_minority / proportion)\n",
    "\n",
    "    # Ensure we do not sample more than available\n",
    "    n_majority = min(n_majority, len(df_majority))\n",
    "\n",
    "    # Randomly sample from the majority class\n",
    "    df_majority_sampled = df_majority.sample(n = n_majority, random_state = 42)\n",
    "\n",
    "    # Combine the minority class with the undersampled majority class\n",
    "    df_balanced = pd.concat([df_majority_sampled, df_minority])\n",
    "\n",
    "    # Shuffle the resulting DataFrame\n",
    "    df_balanced = df_balanced.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def smote(df : pd.DataFrame, response_col_name : str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply SMOTE to oversample the minority class in the dataset and return a balanced DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with balanced data using SMOTE.\n",
    "    \"\"\"\n",
    "    y = df[response_col_name]\n",
    "    X = df.drop(columns=[response_col_name])\n",
    "    \n",
    "    sm = SMOTE(random_state = 42)\n",
    "    X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "    \n",
    "    df_resampled = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_resampled, columns = X.columns), \n",
    "            pd.DataFrame(y_resampled, columns = [response_col_name])\n",
    "        ], \n",
    "        axis = 1)\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "def balance_data(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        prop_synthetic_data : float = 0.25\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply SMOTE and undersampling to return a balanced DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        prop_synthetic_data (float): Proportion of final data that will be synthetic. (0 to 0.5)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with balanced data.\n",
    "    \"\"\"\n",
    "\n",
    "    if prop_synthetic_data < 0:\n",
    "        prop_synthetic_data = 0\n",
    "    elif prop_synthetic_data >= 0.5:\n",
    "        prop_synthetic_data = 0.499\n",
    "\n",
    "    df = undersample_data(\n",
    "        df = df, \n",
    "        response_col_name = response_col_name,\n",
    "        proportion = (1 - 2 * prop_synthetic_data))\n",
    "    \n",
    "    df = smote(df, response_col_name = response_col_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Importance function\n",
    "\n",
    "def get_feature_importance(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        excluded_cols : list[str] = []\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Uses Random Forests to identify the most important features in predicting response.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        excluded_cols (list[str]): Columns to be excluded from the heatmap.\n",
    "        scale (bool): To scale the dataset or not.\n",
    "    \"\"\"\n",
    "\n",
    "    col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in excluded_cols \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    X = df[col_names]\n",
    "    y = df[response_col_name]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size = 0.3, random_state = 42, stratify = y)\n",
    "    \n",
    "    # Initialised the Random Forest Classifier with class weight to handle class imbalance\n",
    "    rf_model = RandomForestClassifier(class_weight = 'balanced', random_state = 42)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for feature importance\n",
    "    features = X.columns  # Get feature names\n",
    "    importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort features by importance\n",
    "    importance_df = importance_df.sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='b')\n",
    "    plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance in Random Forest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Data Processing workflow\n",
    "\n",
    "def data_processing(logs : bool = False) -> pd.DataFrame: \n",
    "    client_df = import_client()\n",
    "    invoice_df = import_invoice()\n",
    "    client_df = convert_date(client_df) # Convert date cols\n",
    "    invoice_df = convert_date(invoice_df)\n",
    "    # Drop duplicates rows\n",
    "    client_df = drop_duplicates(client_df, logs = logs) \n",
    "    invoice_df = drop_duplicates(invoice_df, logs = logs)\n",
    "    categorical_column_names = ['region', 'dis', 'id', 'catg', 'target']\n",
    "    client_df = convert_to_categorical( # Convert categorical cols\n",
    "        client_df, cols = categorical_column_names\n",
    "        )\n",
    "    invoice_df = aggregate_invoice(invoice_df) # Aggregate invoices\n",
    "    invoice_df = manual_fix_names( # Fix column names manually\n",
    "        invoice_df, \n",
    "        new_col_names = [\n",
    "            'id', \n",
    "            'cons_level_1_sum', 'cons_level_1_mean', \n",
    "            'cons_level_1_max', 'cons_level_1_std',\n",
    "            'cons_level_2_sum', 'cons_level_2_mean', \n",
    "            'cons_level_2_max', 'cons_level_2_std',\n",
    "            'cons_level_3_sum', 'cons_level_3_mean', \n",
    "            'cons_level_3_max', 'cons_level_3_std',\n",
    "            'cons_level_4_sum', 'cons_level_4_mean', \n",
    "            'cons_level_4_max', 'cons_level_4_std',\n",
    "            'date_sum', 'date_mean', 'date_max', 'date_std',\n",
    "            'num_invoices'\n",
    "            ]\n",
    "        )\n",
    "    df = merge(client_df = client_df, invoice_df = invoice_df) # Merge\n",
    "    df = prep_dataframe( # Prep for PCA\n",
    "        df = df,\n",
    "        response_col_name = 'target',\n",
    "        cat_col_names = categorical_column_names\n",
    "    )\n",
    "    if logs:\n",
    "        heatmap( # Do Heatmap for pre-reduced data\n",
    "            df, excluded_cols = categorical_column_names)\n",
    "        # Get feature importance for pre-reduced data\n",
    "        get_feature_importance(df, 'target', categorical_column_names)\n",
    "    df = principal_component_analysis( # Do PCA\n",
    "        df = df,\n",
    "        response_col_name = 'target'\n",
    "    )\n",
    "    # Do Low Variance Filter\n",
    "    df = filter_low_variance(df, response_col_name = 'target')\n",
    "    if logs:\n",
    "        heatmap( # Do Heatmap for post-reduced data\n",
    "            df, excluded_cols = categorical_column_names, annotate = True)\n",
    "        # Get feature importance for post-reduced data\n",
    "        get_feature_importance(df, 'target')\n",
    "        # Print pre-balancing proportion of response\n",
    "        print(df['target'].value_counts()) \n",
    "    df = balance_data( # Do balancing \n",
    "        df = df, \n",
    "        response_col_name = 'target', \n",
    "        prop_synthetic_data = 0.4 # Final proportion of synthetic data\n",
    "        ) \n",
    "    if logs:\n",
    "        # Print post-balancing proportion of response\n",
    "        print(df['target'].value_counts()) \n",
    "        print(df.head())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Models\n",
    "\n",
    "def logistic_regression(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[int | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses logistic regression algorithm to do classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = LogisticRegression(random_state = 42)\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def k_nearest_neighbors(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        k : int,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[int | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses k-nearest neighbors algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        k (int): Number of nearest neighbors to consider.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = KNeighborsClassifier(n_neighbors = k)\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def decision_tree(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[int | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses decision tree algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = DecisionTreeClassifier(random_state = 42)\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def random_forest(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[int | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses random forest algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = RandomForestClassifier(random_state = 42)\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def light_gradient_boosting_machine(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[int | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses light gradient boosting machine algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col].astype(int)\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    train_data = lgb.Dataset(X, label = y)\n",
    "    param = {'objective' : 'binary', 'metric' : 'binary_logloss'}\n",
    "    bst = lgb.train(param, train_data, 100)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    y_prob = bst.predict(test_data[feature_names])\n",
    "    if prob:\n",
    "        return y_prob\n",
    "    y_pred = np.array([1 if prob > 0.5 else 0 for prob in y_prob])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[int | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses stochastic gradient descent algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = SGDClassifier(loss = \"log_loss\", random_state = 42)\n",
    "    Model.fit(X, y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def extreme_gradient_boost(train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[int | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses extreme gradient boost algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = xgb.XGBClassifier(random_state = 42)\n",
    "    Model.fit(X, y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:,1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define main workflow\n",
    "\n",
    "def main():\n",
    "    df = data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run to execute main workflow\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0.18645783 0.0109071  0.07334836 ... 0.13643851 0.09285764 0.32260745]\n"
     ]
    }
   ],
   "source": [
    "## FOR TESTING \n",
    "\n",
    "df = data_processing()\n",
    "split_index = len(df) // 2\n",
    "df1 = df.iloc[:split_index]\n",
    "df2 = df.iloc[split_index:]\n",
    "out = logistic_regression(df1, 'target', df2, False)\n",
    "print(out)\n",
    "out = logistic_regression(df1, 'target', df2, True)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
