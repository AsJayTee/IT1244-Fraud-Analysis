{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siah Jin Thau\\OneDrive\\Desktop\\Work\\IT1244\\Group Project\\IT1244-Fraud-Analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Import packages\n",
    "import optuna\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal, Callable\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# To remove warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Importing functions\n",
    "\n",
    "def import_client(filepath : str = \"Data/client.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to import client dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def import_invoice(filepath : str = \"Data/invoice.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to import invoice dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing functions\n",
    "\n",
    "def convert_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts date column to an integer representing \n",
    "    the number of days since the earliest date.\n",
    "    *Column name 'date' is fixed for both datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], dayfirst = True)\n",
    "    \n",
    "    # Find the earliest date\n",
    "    earliest_date = df['date'].min()\n",
    "    \n",
    "    # Calculate the number of days since the earliest date\n",
    "    df['date'] = (df['date'] - earliest_date).dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_duplicates(df : pd.DataFrame, logs : bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prints the result of a duplicate check.\n",
    "    Drops duplicates if they exist.\n",
    "    \"\"\"\n",
    "    if df.duplicated().any(): # Duplicates check\n",
    "        if logs:\n",
    "            print(\"Duplicates found! Cleaning them up...\")\n",
    "        df = df.drop_duplicates() # Drops duplicates\n",
    "        df = df.reset_index(drop = True) # Resets indexes\n",
    "    elif logs:\n",
    "        print(\"No duplicates found!\")\n",
    "    return df\n",
    "\n",
    "def convert_to_categorical(\n",
    "        df : pd.DataFrame,\n",
    "        cols : list[str]\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts list of column names to categorical datatype.\n",
    "    \"\"\"\n",
    "    df[cols] = df[cols].astype('category')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Feature Engineering functions\n",
    "\n",
    "def aggregate_invoice(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate the invoice dataframe by id and generate features.\n",
    "    Calculates sum, mean, max, and std for each consommation_level and date\n",
    "    and counts number of invoices under each id.\n",
    "    \"\"\"\n",
    "    df = df.groupby('id').agg({ # Aggregate by id\n",
    "        # Calculate sum, mean, max, and std for each consm_level and date\n",
    "        'consommation_level_1': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_2': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_3': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_4': ['sum', 'mean', 'max', 'std'],\n",
    "        'date': ['sum', 'mean', 'max', 'std'],\n",
    "        'counter_statue': 'count', # Count number of invoices\n",
    "    }).reset_index()\n",
    "    df.columns = [\n",
    "        # Join by _ if more than 2 parts to the column name exists\n",
    "        '_'.join(col).strip() if col[1] \n",
    "        else col[0] # Else use original name\n",
    "        for col in df.columns.values # For each column name value\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def manual_fix_names(\n",
    "        df : pd.DataFrame,\n",
    "        new_col_names : list[str]\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Manually sets the column names of a dataframe.\n",
    "    \"\"\"\n",
    "    df.columns = new_col_names\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Joining functions\n",
    "\n",
    "def merge(\n",
    "        client_df : pd.DataFrame,\n",
    "        invoice_df : pd.DataFrame,\n",
    "        merge_by : str = \"id\"\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges two dataframes.\n",
    "    Merges on 'id' column by default (for client and invoice).\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(\n",
    "        client_df, invoice_df, on = merge_by)\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensionality Reduction functions\n",
    "\n",
    "def prep_dataframe(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [],\n",
    "        scale : bool = True,\n",
    "        OHE : bool = True\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For scaling and one-hot encoding dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        scale (bool): Indicate if data should be scaled\n",
    "        OHE (bool): Indicate if categorical variables should undergo One-Hot Encoding\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns scaled/One-Hot Encoded\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.drop(columns = ['id']) \n",
    "    # Drop id column as it is not useful for predicting\n",
    "\n",
    "    cat_col_names.remove('id') \n",
    "    # Remove 'id' as it no longer exists in dataframe\n",
    "\n",
    "    if response_col_name in cat_col_names:\n",
    "        cat_col_names.remove(response_col_name)\n",
    "    # Exclude doing OHE on response column\n",
    "\n",
    "    # Identify numerical columns \n",
    "    # by excluding categorical and response columns\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in cat_col_names \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    if scale:\n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "    if OHE:\n",
    "        # One-hot encode categorical features\n",
    "        encoder = OneHotEncoder(sparse_output = False, drop = 'first')  \n",
    "        # drop = 'first' to avoid dummy variable trap\n",
    "        X_cat = encoder.fit_transform(X_cat)\n",
    "    \n",
    "    # Combine the scaled numerical and encoded categorical features\n",
    "    X_prep = pd.DataFrame(\n",
    "        data = np.hstack((X_num, X_cat)),  # Horizontal stack to combine arrays\n",
    "        columns = num_col_names + list(encoder.get_feature_names_out(cat_col_names))\n",
    "    )\n",
    "\n",
    "    # Create a new DataFrame including the response variable\n",
    "    prep_df = pd.DataFrame(X_prep, columns = X_prep.columns)\n",
    "    prep_df[response_col_name] = y\n",
    "\n",
    "    return prep_df\n",
    "\n",
    "def principal_component_analysis(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [], \n",
    "        var : float = 0.95, \n",
    "        logs : bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For reducing the dimensions of a dataframe using PCA.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        var (float): Proportion of variance that should be preserved\n",
    "        logs (bool): Indicate if logs should be printed\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame that has undergone PCA\n",
    "    \"\"\"\n",
    "\n",
    "    if response_col_name in cat_col_names:\n",
    "        cat_col_names.remove(response_col_name)\n",
    "    # Exclude doing OHE on response column\n",
    "\n",
    "    # Identify numerical columns \n",
    "    # by excluding categorical and response columns\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in cat_col_names \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    pca = PCA(n_components = var) \n",
    "    # Keep 'var' proportion of the variance : default 95%\n",
    "    X_pca = pca.fit_transform(X_num) \n",
    "\n",
    "    pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "    df_pca = pd.DataFrame(X_pca, columns = pca_columns)\n",
    "\n",
    "    df_modified = pd.concat(\n",
    "        [\n",
    "            df_pca, \n",
    "            pd.DataFrame(X_cat, columns = cat_col_names), \n",
    "            pd.Series(y, name = response_col_name)\n",
    "        ], \n",
    "        axis = 1)\n",
    "    \n",
    "    if logs:\n",
    "        print(f\"Number of components selected: {pca.n_components_}\")\n",
    "        print(\"Explained variance ratio for each component:\", pca.explained_variance_ratio_)\n",
    "        print(\"Cumulative explained variance:\", pca.explained_variance_ratio_.cumsum())\n",
    "        print(\"Final DataFrame with PCA applied to numeric columns:\")\n",
    "        print(df_modified.head())\n",
    "\n",
    "    return df_modified\n",
    "\n",
    "def filter_low_variance(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        threshold: float = 0.1\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter numeric columns based on a variance threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        threshold (float): The variance threshold for filtering columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with low variance numeric columns removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify numerical columns \n",
    "    # by excluding response column\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col != response_col_name\n",
    "    ]\n",
    "\n",
    "    # Calculate the variance for each numeric column\n",
    "    variances = df[num_col_names].var()\n",
    "\n",
    "    # Keep response column\n",
    "    high_variance_cols = [response_col_name]\n",
    "\n",
    "    # Filter columns with variance greater than the threshold : default 0.1\n",
    "    cols_to_keep = variances[variances > threshold].index.tolist()\n",
    "    high_variance_cols.extend(cols_to_keep)\n",
    "\n",
    "    # Create a new DataFrame with only the selected columns\n",
    "    filtered_df = df[high_variance_cols]\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualisation functions\n",
    "\n",
    "def heatmap(\n",
    "        df : pd.DataFrame,\n",
    "        excluded_cols : list[str],\n",
    "        annotate : bool = False\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Plots a Correlation Heatmap.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        excluded_cols (list[str]): Columns to be excluded from the heatmap.\n",
    "        annotate (float): To annotate the heatmap with correlation values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify desired columns \n",
    "    # by excluding undesired column\n",
    "    col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in excluded_cols\n",
    "    ]\n",
    "\n",
    "    X = df[col_names]\n",
    "\n",
    "    corr_matrix = X.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        corr_matrix, annot = annotate, cmap = 'coolwarm', fmt = '.2f'\n",
    "    )\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Balancing functions\n",
    "\n",
    "def undersample_data(\n",
    "        df: pd.DataFrame, \n",
    "        response_col_name: str, \n",
    "        proportion: float = 1\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform undersampling to balance the classes in the dataset based on a specified proportion.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        proportion (float): The desired ratio of minority to majority class samples (e.g., 0.5 for 1:2).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A balanced DataFrame with samples from both classes according to the specified proportion.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the majority and minority classes\n",
    "    majority_class = df[response_col_name].value_counts().idxmax()\n",
    "    minority_class = df[response_col_name].value_counts().idxmin()\n",
    "\n",
    "    # Split the DataFrame into majority and minority\n",
    "    df_majority = df[df[response_col_name] == majority_class]\n",
    "    df_minority = df[df[response_col_name] == minority_class]\n",
    "\n",
    "    # Calculate the number of samples to take from the majority class\n",
    "    n_minority = len(df_minority)\n",
    "    n_majority = int(n_minority / proportion)\n",
    "\n",
    "    # Ensure we do not sample more than available\n",
    "    n_majority = min(n_majority, len(df_majority))\n",
    "\n",
    "    # Randomly sample from the majority class\n",
    "    df_majority_sampled = df_majority.sample(n = n_majority, random_state = 42)\n",
    "\n",
    "    # Combine the minority class with the undersampled majority class\n",
    "    df_balanced = pd.concat([df_majority_sampled, df_minority])\n",
    "\n",
    "    # Shuffle the resulting DataFrame\n",
    "    df_balanced = df_balanced.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def smote(df : pd.DataFrame, response_col_name : str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply SMOTE to oversample the minority class in the dataset and return a balanced DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with balanced data using SMOTE.\n",
    "    \"\"\"\n",
    "    y = df[response_col_name]\n",
    "    X = df.drop(columns=[response_col_name])\n",
    "    \n",
    "    sm = SMOTE(random_state = 42)\n",
    "    X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "    \n",
    "    df_resampled = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_resampled, columns = X.columns), \n",
    "            pd.DataFrame(y_resampled, columns = [response_col_name])\n",
    "        ], \n",
    "        axis = 1)\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "def balance_data(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        prop_synthetic_data : float = 0.25\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply SMOTE and undersampling to return a balanced DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        prop_synthetic_data (float): Proportion of final data that will be synthetic. (0 to 0.5)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with balanced data.\n",
    "    \"\"\"\n",
    "\n",
    "    if prop_synthetic_data < 0:\n",
    "        prop_synthetic_data = 0\n",
    "    elif prop_synthetic_data >= 0.5:\n",
    "        prop_synthetic_data = 0.499\n",
    "\n",
    "    df = undersample_data(\n",
    "        df = df, \n",
    "        response_col_name = response_col_name,\n",
    "        proportion = (1 - 2 * prop_synthetic_data))\n",
    "    \n",
    "    df = smote(df, response_col_name = response_col_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Importance function\n",
    "\n",
    "def get_feature_importance(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        excluded_cols : list[str] = []\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Uses Random Forests to identify the most important features in predicting response.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        response_col_name (str): The name of the target column (class label).\n",
    "        excluded_cols (list[str]): Columns to be excluded from the heatmap.\n",
    "        scale (bool): To scale the dataset or not.\n",
    "    \"\"\"\n",
    "\n",
    "    col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in excluded_cols \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    X = df[col_names]\n",
    "    y = df[response_col_name]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size = 0.3, random_state = 42, stratify = y)\n",
    "    \n",
    "    # Initialised the Random Forest Classifier with class weight to handle class imbalance\n",
    "    rf_model = RandomForestClassifier(class_weight = 'balanced', random_state = 42)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for feature importance\n",
    "    features = X.columns  # Get feature names\n",
    "    importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort features by importance\n",
    "    importance_df = importance_df.sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='b')\n",
    "    plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance in Random Forest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Data Processing workflow\n",
    "\n",
    "def data_processing(logs : bool = False) -> pd.DataFrame: \n",
    "    client_df = import_client()\n",
    "    invoice_df = import_invoice()\n",
    "    client_df = convert_date(client_df) # Convert date cols\n",
    "    invoice_df = convert_date(invoice_df)\n",
    "    # Drop duplicates rows\n",
    "    client_df = drop_duplicates(client_df, logs = logs) \n",
    "    invoice_df = drop_duplicates(invoice_df, logs = logs)\n",
    "    categorical_column_names = ['region', 'dis', 'id', 'catg', 'target']\n",
    "    client_df = convert_to_categorical( # Convert categorical cols\n",
    "        client_df, cols = categorical_column_names\n",
    "        )\n",
    "    invoice_df = aggregate_invoice(invoice_df) # Aggregate invoices\n",
    "    invoice_df = manual_fix_names( # Fix column names manually\n",
    "        invoice_df, \n",
    "        new_col_names = [\n",
    "            'id', \n",
    "            'cons_level_1_sum', 'cons_level_1_mean', \n",
    "            'cons_level_1_max', 'cons_level_1_std',\n",
    "            'cons_level_2_sum', 'cons_level_2_mean', \n",
    "            'cons_level_2_max', 'cons_level_2_std',\n",
    "            'cons_level_3_sum', 'cons_level_3_mean', \n",
    "            'cons_level_3_max', 'cons_level_3_std',\n",
    "            'cons_level_4_sum', 'cons_level_4_mean', \n",
    "            'cons_level_4_max', 'cons_level_4_std',\n",
    "            'date_sum', 'date_mean', 'date_max', 'date_std',\n",
    "            'num_invoices'\n",
    "            ]\n",
    "        )\n",
    "    df = merge(client_df = client_df, invoice_df = invoice_df) # Merge\n",
    "    df = prep_dataframe( # Prep for PCA\n",
    "        df = df,\n",
    "        response_col_name = 'target',\n",
    "        cat_col_names = categorical_column_names\n",
    "    )\n",
    "    if logs:\n",
    "        heatmap( # Do Heatmap for pre-reduced data\n",
    "            df, excluded_cols = categorical_column_names)\n",
    "        # Get feature importance for pre-reduced data\n",
    "        get_feature_importance(df, 'target', categorical_column_names)\n",
    "    df = principal_component_analysis( # Do PCA\n",
    "        df = df,\n",
    "        response_col_name = 'target'\n",
    "    )\n",
    "    # Do Low Variance Filter\n",
    "    df = filter_low_variance(df, response_col_name = 'target')\n",
    "    if logs:\n",
    "        heatmap( # Do Heatmap for post-reduced data\n",
    "            df, excluded_cols = categorical_column_names, annotate = True)\n",
    "        # Get feature importance for post-reduced data\n",
    "        get_feature_importance(df, 'target')\n",
    "        # Print pre-balancing proportion of response\n",
    "        print(df['target'].value_counts()) \n",
    "    df = balance_data( # Do balancing \n",
    "        df = df, \n",
    "        response_col_name = 'target', \n",
    "        prop_synthetic_data = 0.4 # Final proportion of synthetic data\n",
    "        ) \n",
    "    if logs:\n",
    "        # Print post-balancing proportion of response\n",
    "        print(df['target'].value_counts()) \n",
    "        print(df.head())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Models\n",
    "\n",
    "def logistic_regression(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False\n",
    "    ) -> np.ndarray[np.int64 | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses logistic regression algorithm to do classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = LogisticRegression(random_state = 42)\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def k_nearest_neighbors(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False,\n",
    "        k : int = 5,\n",
    "        weights : Literal['uniform', 'distance'] = 'uniform',\n",
    "        power_parameter : int = 2\n",
    "    ) -> np.ndarray[np.int64 | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses k-nearest neighbors algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        k (int): Number of nearest neighbors to consider.\n",
    "        weights ({'uniform', 'distance'}): Weight function used in prediction.\n",
    "        power_parameter (int): Power parameter for the Minkowski metric. \n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = KNeighborsClassifier(\n",
    "        n_neighbors = k,\n",
    "        weights = weights,\n",
    "        p = power_parameter\n",
    "    )\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def decision_tree(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False,\n",
    "        criterion : Literal['gini', 'entropy', 'log_loss'] = 'gini',\n",
    "        splitter : Literal['best', 'random'] = 'best',\n",
    "        max_depth : int = None,\n",
    "        min_samples_split : float = None,\n",
    "        min_samples_leaf : float = None\n",
    "    ) -> np.ndarray[np.int64 | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses decision tree algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        criterion ({'gini', 'entropy', 'log_loss'}): The function to measure the quality of a split.\n",
    "        splitter ({'best', 'random'}): The strategy used to choose the split at each node.\n",
    "        max_depth (int): The maximum depth of the tree.\n",
    "        min_samples_split (float 0 < x < 1): Fraction of total sample representing minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf (float 0 < x < 1): Fraction of total sample representing minimum number of samples for each node.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    if min_samples_split is None:\n",
    "        min_samples_split = 2\n",
    "\n",
    "    if min_samples_leaf is None:\n",
    "        min_samples_leaf = 1\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = DecisionTreeClassifier(\n",
    "        criterion = criterion,\n",
    "        splitter = splitter,\n",
    "        max_depth = max_depth,\n",
    "        min_samples_split = min_samples_split,\n",
    "        min_samples_leaf = min_samples_leaf,\n",
    "        random_state = 42\n",
    "    )\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def random_forest(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False,\n",
    "        n_estimators : int = 100,\n",
    "        criterion : Literal['gini', 'entropy', 'log_loss'] = 'gini',\n",
    "        max_depth : int = None,\n",
    "        min_samples_split : float = None,\n",
    "        min_samples_leaf : float = None\n",
    "    ) -> np.ndarray[np.int64 | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses random forest algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        n_estimators (int): The number of trees in the forest.\n",
    "        criterion ({'gini', 'entropy', 'log_loss'}): The function to measure the quality of a split.\n",
    "        max_depth (int): The maximum depth of the tree.\n",
    "        min_samples_split (float 0 < x < 1): Fraction of total sample representing minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf (float 0 < x < 1): Fraction of total sample representing minimum number of samples for each node.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    if min_samples_split is None:\n",
    "        min_samples_split = 2\n",
    "\n",
    "    if min_samples_leaf is None:\n",
    "        min_samples_leaf = 1\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = RandomForestClassifier(\n",
    "        n_estimators = n_estimators,\n",
    "        criterion = criterion,\n",
    "        max_depth = max_depth,\n",
    "        min_samples_split = min_samples_split,\n",
    "        min_samples_leaf = min_samples_leaf,\n",
    "        random_state = 42\n",
    "    )\n",
    "    Model.fit(X = X, y = y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def light_gradient_boosting_machine(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False,\n",
    "        num_iterations : int = 100,\n",
    "        learning_rate : float = 0.1,\n",
    "        num_leaves : int = 31,\n",
    "        max_depth : int = -1,\n",
    "        bagging_freq : int = 0,\n",
    "        bagging_fraction : float = 1,\n",
    "        feature_fraction : float = 1,\n",
    "        lambda_l1 : float = 0,\n",
    "        lambda_l2 : float = 0,\n",
    "        min_split_gain : float = 0\n",
    "    ) -> np.ndarray[np.int64 | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses light gradient boosting machine algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        num_iterations (int): The number of boosting iterations.\n",
    "        learning_rate (float > 0): Shrinkage rate.\n",
    "        num_leaves (int 1 < x <= 131072): The max number of leaves in one tree.\n",
    "        max_depth (int): Limit the max depth for tree model. <= 0 means no limit.\n",
    "        bagging_freq (int >= 0): Frequency for bagging.\n",
    "        bagging_fraction (float 0 < x <= 1): To randomly selecting data without resampling.\n",
    "        feature_fraction (float 0 < x <= 1): To randomly selecting data without resampling.\n",
    "        lambda_l1 (float >= 0): L1 regularization.\n",
    "        lambda_l2 (float >= 0): L2 regularization.\n",
    "        min_split_gain (float >= 0): The minimal gain to perform split.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col].astype(int)\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    train_data = lgb.Dataset(X, label = y)\n",
    "    param = {\n",
    "        'objective' : 'binary', \n",
    "        'metric' : 'binary_logloss',\n",
    "        'boosting_type' : 'gbdt',\n",
    "        'num_iterations' : num_iterations,\n",
    "        'learning_rate' : learning_rate,\n",
    "        'num_leaves' : num_leaves,\n",
    "        'max_depth' : max_depth,\n",
    "        'bagging_freq' : bagging_freq,\n",
    "        'bagging_fraction' : bagging_fraction,\n",
    "        'feature_fraction' : feature_fraction,\n",
    "        'lambda_l1' : lambda_l1,\n",
    "        'lambda_l2' : lambda_l2,\n",
    "        'min_split_gain': min_split_gain,\n",
    "        'deterministic' : True,\n",
    "        'verbosity' : -1\n",
    "    }\n",
    "    bst = lgb.train(param, train_data, 100)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    y_prob = bst.predict(test_data[feature_names])\n",
    "    if prob:\n",
    "        return y_prob\n",
    "    y_pred = np.array([1 if prob > 0.5 else 0 for prob in y_prob])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False,\n",
    "        penalty : Literal['l2', 'l1', 'elasticnet', None] = 'l2',\n",
    "        alpha : float = 0.0001,\n",
    "        max_iter : int = 100,\n",
    "        learning_rate : Literal['constant', 'optimal', 'invscaling', 'adaptive'] = 'optimal'\n",
    "    ) -> np.ndarray[np.int64 | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses stochastic gradient descent algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        penalty ({'l2', 'l1', 'elasticnet', None}): The penalty (aka regularization term) to be used. \n",
    "        alpha (float > 0): Constant that multiplies the regularization term.\n",
    "        max_iter (int): The maximum number of passes over the training data (aka epochs).\n",
    "        learning_rate ({'constant', 'optimal', 'invscaling', 'adaptive'}): The learning rate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = SGDClassifier(\n",
    "        loss = \"log_loss\", \n",
    "        penalty = penalty,\n",
    "        alpha = alpha,\n",
    "        max_iter = max_iter,\n",
    "        learning_rate = learning_rate,\n",
    "        random_state = 42)\n",
    "    Model.fit(X, y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:, 1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def extreme_gradient_boost(\n",
    "        train_data : pd.DataFrame,\n",
    "        response_col : str,\n",
    "        test_data : pd.DataFrame,\n",
    "        prob : bool = False,\n",
    "        learning_rate : float = 0.3,\n",
    "        min_split_loss : float = 0,\n",
    "        max_depth : int = 6,\n",
    "        max_delta_step : int = 0,\n",
    "        subsample : float = 1,\n",
    "        sampling_method : Literal['uniform', 'gradient_based'] = 'uniform',\n",
    "        reg_lambda : float = 1,\n",
    "        reg_alpha : float = 0,\n",
    "        tree_method : Literal['auto', 'exact', 'approx', 'hist'] = 'auto'\n",
    "    ) -> np.ndarray[np.int64 | np.float64]:\n",
    "    \"\"\"\n",
    "    Uses extreme gradient boost algorithm to perform classification.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (pd.DataFrame): The input DataFrame containing the train data.\n",
    "        response_col (str): The name of the target column (class label).\n",
    "        test_data (pd.DataFrame): The input DataFrame containing the test data.\n",
    "        prob (bool): To return a vector of probabilities instead of class predictions.\n",
    "        learning_rate (float 0 <= x <= 1): Step size shrinkage used in update to prevent overfitting.\n",
    "        min_split_loss (float >= 0): Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "        max_depth (int): Maximum depth of a tree. Beware that XGBoost aggressively consumes memory when training a deep tree. \"exact\" tree method requires non-zero value.\n",
    "        max_delta_step (int >= 0): Maximum delta step we allow each leaf output to be. Setting it to value of 1-10 might help control the update.\n",
    "        subsample (int 0 < x <= 1): Subsample ratio of the training instances.\n",
    "        sampling_method ({'uniform', 'gradient_based'}): The method to use to sample the training instances.\n",
    "        reg_lambda (float >= 0): L2 regularization term on weights. \n",
    "        reg_alpha (float >= 0): L1 regularization term on weights. \n",
    "        tree_method ({'auto', 'exact', 'approx', 'hist'}): The tree construction algorithm used in XGBoost.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array with model predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_data[response_col]\n",
    "    X = train_data.drop(columns = [response_col])\n",
    "\n",
    "    Model = xgb.XGBClassifier(\n",
    "        eta = learning_rate,\n",
    "        gamma = min_split_loss,\n",
    "        max_depth = max_depth,\n",
    "        max_delta_step = max_delta_step,\n",
    "        subsample = subsample,\n",
    "        sampling_method = sampling_method,\n",
    "        reg_lambda = reg_lambda,\n",
    "        reg_alpha = reg_alpha,\n",
    "        tree_method = tree_method,\n",
    "        random_state = 42\n",
    "    )\n",
    "    Model.fit(X, y)\n",
    "\n",
    "    feature_names = X.columns\n",
    "    if prob:\n",
    "        y_pred = Model.predict_proba(test_data[feature_names])[:,1]\n",
    "    else:\n",
    "        y_pred = Model.predict(test_data[feature_names])\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning functions\n",
    "\n",
    "def cross_validate_model(\n",
    "        model : Callable,\n",
    "        params : dict,\n",
    "        data : pd.DataFrame = data_processing(),\n",
    "        response_col : str = 'target',\n",
    "        n_folds : int = 5\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross-validation on a specific model,\n",
    "    ensuring the ratio of positive and negative cases is balanced across all folds.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The full dataset to split into folds.\n",
    "        response_col (str): The name of the response column.\n",
    "        params (dict): The parameters for the model.\n",
    "        n_folds (int): The number of folds for cross-validation.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing average cross-validated metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 42)\n",
    "    \n",
    "    recall_scores = []\n",
    "    \n",
    "    X = data.drop(columns = [response_col])\n",
    "    y = data[response_col]\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        train_data = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "        test_data = X_test\n",
    "        \n",
    "        y_pred = model(\n",
    "            train_data = train_data, \n",
    "            response_col = response_col, \n",
    "            test_data = test_data,\n",
    "            prob = False,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        recall_scores.append(recall_score(y_test, y_pred, zero_division = 0))\n",
    "    \n",
    "    recall = np.mean(recall_scores)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def get_optimal_parameters(\n",
    "        model : Callable,\n",
    "        int_params : dict[str : list] = {},\n",
    "        float_params : dict[str : list] = {},\n",
    "        categorical_params : dict[str : list] = {},\n",
    "        trials : int = 20\n",
    "        ):\n",
    "\n",
    "    def helper(\n",
    "            trial : optuna.trial.Trial,\n",
    "        ) -> dict:\n",
    "\n",
    "        params = {}\n",
    "\n",
    "        for argument, values in int_params.items():\n",
    "            params[argument] = trial.suggest_int(argument, values[0], values[1])\n",
    "        \n",
    "        for argument, values in float_params.items():\n",
    "            params[argument] = trial.suggest_float(argument, values[0], values[1])\n",
    "\n",
    "        for argument, values in categorical_params.items():\n",
    "            params[argument] = trial.suggest_categorical(argument, values)\n",
    "\n",
    "        recall = cross_validate_model(\n",
    "            model = model,\n",
    "            params = params\n",
    "        )\n",
    "\n",
    "        return(recall)\n",
    "\n",
    "    study = optuna.create_study(direction = 'maximize')\n",
    "    study.optimize(func = helper, n_trials = trials)\n",
    "    return(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define main workflow\n",
    "\n",
    "def main():\n",
    "    df = data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run to execute main workflow\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-25 11:35:34,655] A new study created in memory with name: no-name-8fa00331-86a9-4de7-810f-c53db60a513d\n",
      "[I 2024-10-25 11:35:35,147] Trial 0 finished with value: 0.8433085501858736 and parameters: {'num_iterations': 125, 'num_leaves': 47, 'max_depth': 15, 'bagging_freq': 4, 'learning_rate': 0.44941056593320144, 'bagging_fraction': 0.8366631481382854, 'feature_fraction': 0.2824151813841448, 'lambda_l1': 3.093668648684237, 'lambda_l2': 1.6053308362939156, 'min_split_gain': 0.3941113237774402}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:35,579] Trial 1 finished with value: 0.5137546468401487 and parameters: {'num_iterations': 193, 'num_leaves': 44, 'max_depth': 24, 'bagging_freq': 3, 'learning_rate': 7.610558993795522, 'bagging_fraction': 0.5137357739810291, 'feature_fraction': 0.8213061493716085, 'lambda_l1': 1.0604475047366595, 'lambda_l2': 3.8820102600026725, 'min_split_gain': 3.1743073598836484}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:35,780] Trial 2 finished with value: 0.2860594795539034 and parameters: {'num_iterations': 172, 'num_leaves': 3, 'max_depth': 28, 'bagging_freq': 5, 'learning_rate': 7.642446080249992, 'bagging_fraction': 0.8630989031509342, 'feature_fraction': 0.41933622205342225, 'lambda_l1': 3.389242352612076, 'lambda_l2': 3.012097036328283, 'min_split_gain': 1.2353847249179966}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:35,959] Trial 3 finished with value: 0.40501858736059476 and parameters: {'num_iterations': 115, 'num_leaves': 10, 'max_depth': 48, 'bagging_freq': 0, 'learning_rate': 8.831747649130104, 'bagging_fraction': 0.1686404076792194, 'feature_fraction': 0.03723822365346581, 'lambda_l1': 0.24770543518066734, 'lambda_l2': 3.922751516652354, 'min_split_gain': 4.3060865174677705}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:36,182] Trial 4 finished with value: 0.7819702602230484 and parameters: {'num_iterations': 189, 'num_leaves': 3, 'max_depth': 38, 'bagging_freq': 0, 'learning_rate': 9.056388680580186, 'bagging_fraction': 0.3533237128782078, 'feature_fraction': 0.41914855056425626, 'lambda_l1': 1.3102893989373332, 'lambda_l2': 4.785843007285114, 'min_split_gain': 2.7016132151538526}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:36,316] Trial 5 finished with value: 0.216728624535316 and parameters: {'num_iterations': 66, 'num_leaves': 31, 'max_depth': 19, 'bagging_freq': 9, 'learning_rate': 7.987036535318065, 'bagging_fraction': 0.7153347472489461, 'feature_fraction': 0.05701360160968178, 'lambda_l1': 4.900358575990251, 'lambda_l2': 1.4665187933599295, 'min_split_gain': 2.321476076443203}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:36,758] Trial 6 finished with value: 0.3104089219330855 and parameters: {'num_iterations': 163, 'num_leaves': 29, 'max_depth': 35, 'bagging_freq': 1, 'learning_rate': 6.074140119015227, 'bagging_fraction': 0.3498769948044911, 'feature_fraction': 0.4913205191444081, 'lambda_l1': 3.499784888943893, 'lambda_l2': 1.011242698007938, 'min_split_gain': 0.9383324082985511}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:36,927] Trial 7 finished with value: 0.6035315985130111 and parameters: {'num_iterations': 114, 'num_leaves': 40, 'max_depth': 25, 'bagging_freq': 0, 'learning_rate': 8.765011539810896, 'bagging_fraction': 0.119411534833166, 'feature_fraction': 0.05554685547582655, 'lambda_l1': 3.70144995307147, 'lambda_l2': 1.4927000039050213, 'min_split_gain': 3.209800033657377}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:37,161] Trial 8 finished with value: 0.4 and parameters: {'num_iterations': 152, 'num_leaves': 31, 'max_depth': 24, 'bagging_freq': 9, 'learning_rate': 8.362053606571688, 'bagging_fraction': 0.42550623256115006, 'feature_fraction': 0.12626830373431858, 'lambda_l1': 1.003664871112457, 'lambda_l2': 3.0491401283489634, 'min_split_gain': 0.554052938546748}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:37,413] Trial 9 finished with value: 0.7535315985130111 and parameters: {'num_iterations': 137, 'num_leaves': 45, 'max_depth': 2, 'bagging_freq': 2, 'learning_rate': 0.7255380672020888, 'bagging_fraction': 0.73591527608445, 'feature_fraction': 0.8087068311881217, 'lambda_l1': 0.17149149105950778, 'lambda_l2': 1.4924047314706057, 'min_split_gain': 4.4338489820107725}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:37,835] Trial 10 finished with value: 0.8358736059479555 and parameters: {'num_iterations': 77, 'num_leaves': 19, 'max_depth': 9, 'bagging_freq': 6, 'learning_rate': 0.42266965467963097, 'bagging_fraction': 0.9423345826132644, 'feature_fraction': 0.2713400151362953, 'lambda_l1': 2.315242942429177, 'lambda_l2': 0.036516558345791816, 'min_split_gain': 0.1658141218916689}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:38,200] Trial 11 finished with value: 0.82453531598513 and parameters: {'num_iterations': 71, 'num_leaves': 17, 'max_depth': 9, 'bagging_freq': 6, 'learning_rate': 0.23616499805749605, 'bagging_fraction': 0.9436868867169923, 'feature_fraction': 0.27032210933269496, 'lambda_l1': 2.319646240788825, 'lambda_l2': 0.0988836156651074, 'min_split_gain': 0.009416575550633244}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:38,467] Trial 12 finished with value: 0.5914498141263941 and parameters: {'num_iterations': 83, 'num_leaves': 20, 'max_depth': 12, 'bagging_freq': 7, 'learning_rate': 2.4630373879410854, 'bagging_fraction': 0.9821828990727075, 'feature_fraction': 0.2821782117691652, 'lambda_l1': 2.3859891867780254, 'lambda_l2': 0.0018811868393843814, 'min_split_gain': 1.6645647440579494}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:38,749] Trial 13 finished with value: 0.5234200743494424 and parameters: {'num_iterations': 94, 'num_leaves': 21, 'max_depth': 12, 'bagging_freq': 4, 'learning_rate': 2.966314110520182, 'bagging_fraction': 0.7370461950026124, 'feature_fraction': 0.257598203821051, 'lambda_l1': 2.85445460916494, 'lambda_l2': 0.7097478314154144, 'min_split_gain': 0.026887651155146824}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:39,116] Trial 14 finished with value: 0.7466542750929369 and parameters: {'num_iterations': 54, 'num_leaves': 50, 'max_depth': 6, 'bagging_freq': 7, 'learning_rate': 2.0256083578630655, 'bagging_fraction': 0.595363958677446, 'feature_fraction': 0.6101720940453305, 'lambda_l1': 1.8007283641420277, 'lambda_l2': 2.259114499029489, 'min_split_gain': 1.8216886549977032}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:39,508] Trial 15 finished with value: 0.583271375464684 and parameters: {'num_iterations': 100, 'num_leaves': 36, 'max_depth': 17, 'bagging_freq': 4, 'learning_rate': 4.512636502844988, 'bagging_fraction': 0.8127149323539548, 'feature_fraction': 0.61752476821189, 'lambda_l1': 4.370722121796882, 'lambda_l2': 2.1323794786206056, 'min_split_gain': 0.6567293841164953}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:39,793] Trial 16 finished with value: 0.7444237918215613 and parameters: {'num_iterations': 136, 'num_leaves': 15, 'max_depth': 2, 'bagging_freq': 7, 'learning_rate': 1.4697337244231068, 'bagging_fraction': 0.615938908832554, 'feature_fraction': 0.18460924065799136, 'lambda_l1': 2.8136067391653468, 'lambda_l2': 0.5954954889189366, 'min_split_gain': 1.3787387067926065}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:40,119] Trial 17 finished with value: 0.6689591078066914 and parameters: {'num_iterations': 101, 'num_leaves': 24, 'max_depth': 17, 'bagging_freq': 10, 'learning_rate': 3.774865988325176, 'bagging_fraction': 0.9959300786669617, 'feature_fraction': 0.36469258628217993, 'lambda_l1': 1.8505440173936005, 'lambda_l2': 1.9371500239068153, 'min_split_gain': 0.5182215600355571}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:40,419] Trial 18 finished with value: 0.4767657992565056 and parameters: {'num_iterations': 128, 'num_leaves': 12, 'max_depth': 7, 'bagging_freq': 5, 'learning_rate': 5.962981497566616, 'bagging_fraction': 0.8401349865590014, 'feature_fraction': 0.9651758568915669, 'lambda_l1': 4.115647435300483, 'lambda_l2': 2.7959534693585777, 'min_split_gain': 0.04157643820677123}. Best is trial 0 with value: 0.8433085501858736.\n",
      "[I 2024-10-25 11:35:40,669] Trial 19 finished with value: 0.7886617100371748 and parameters: {'num_iterations': 50, 'num_leaves': 36, 'max_depth': 14, 'bagging_freq': 2, 'learning_rate': 1.1472992394526502, 'bagging_fraction': 0.8911149499296187, 'feature_fraction': 0.5396627258946216, 'lambda_l1': 2.985762220693074, 'lambda_l2': 0.5152302270187044, 'min_split_gain': 2.0448182230599334}. Best is trial 0 with value: 0.8433085501858736.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_iterations': 125,\n",
       " 'num_leaves': 47,\n",
       " 'max_depth': 15,\n",
       " 'bagging_freq': 4,\n",
       " 'learning_rate': 0.44941056593320144,\n",
       " 'bagging_fraction': 0.8366631481382854,\n",
       " 'feature_fraction': 0.2824151813841448,\n",
       " 'lambda_l1': 3.093668648684237,\n",
       " 'lambda_l2': 1.6053308362939156,\n",
       " 'min_split_gain': 0.3941113237774402}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FOR TESTING \n",
    "\n",
    "get_optimal_parameters(\n",
    "    light_gradient_boosting_machine,\n",
    "    int_params = {\n",
    "        'num_iterations' : [50, 200],\n",
    "        'num_leaves' : [1, 50],\n",
    "        'max_depth' : [1, 50],\n",
    "        'bagging_freq' : [0, 10]\n",
    "    },\n",
    "    float_params = {\n",
    "        'learning_rate' : [0.001, 10],\n",
    "        'bagging_fraction' : [0.001, 1],\n",
    "        'feature_fraction' : [0.001, 1],\n",
    "        'lambda_l1' : [0, 5],\n",
    "        'lambda_l2' : [0, 5],\n",
    "        'min_split_gain' : [0, 5]\n",
    "    },\n",
    "    categorical_params = {}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
