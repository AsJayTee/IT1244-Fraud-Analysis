{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Data/{filename}.csv\" # TO UPDATE after feature engineering is complete\n",
    "\n",
    "# Column names from feature engineered dataset\n",
    "response_col_name = \"\" # Response col\n",
    "cat_col_names = [] # Categorical col \n",
    "num_col_names = [] # Numerical col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [], \n",
    "        num_col_names : list[str] = [],\n",
    "        scale : bool = True,\n",
    "        OHE : bool = True\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For scaling and one-hot encoding dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        num_col_names (list[str]): List of numeric column names to evaluate.\n",
    "        scale (bool): Indicate if data should be scaled\n",
    "        OHE (bool): Indicate if categorical variables should undergo One-Hot Encoding\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns scaled/One-Hot Encoded\n",
    "    \"\"\"\n",
    "    \n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    if scale:\n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "    if OHE:\n",
    "        # One-hot encode categorical features\n",
    "        encoder = OneHotEncoder(sparse = False, drop = 'first')  # drop = 'first' to avoid dummy variable trap\n",
    "        X_cat = encoder.fit_transform(X_cat)\n",
    "    \n",
    "    # Combine the scaled numerical and encoded categorical features\n",
    "    X_preprocessed = pd.DataFrame(\n",
    "        data = np.hstack((X_num, X_cat)),  # Horizontal stack to combine arrays\n",
    "        columns = num_col_names + list(encoder.get_feature_names_out(cat_col_names))\n",
    "    )\n",
    "\n",
    "    # Create a new DataFrame including the response variable\n",
    "    preprocessed_df = pd.DataFrame(X_preprocessed, columns = X_preprocessed.columns)\n",
    "    preprocessed_df[response_col_name] = y\n",
    "\n",
    "    return preprocessed_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_analysis(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [], \n",
    "        num_col_names : list[str] = [],\n",
    "        var : float = 0.95, \n",
    "        logs : bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For reducing the dimensions of a dataframe using PCA.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        num_col_names (list[str]): List of numeric column names to evaluate.\n",
    "        var (float): Proportion of variance that should be preserved\n",
    "        logs (bool): Indicate if logs should be printed\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame that has undergone PCA\n",
    "    \"\"\"\n",
    "\n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    pca = PCA(n_components = var) # Keep 'var' proportion of the variance : default 95%\n",
    "    X_pca = pca.fit_transform(X_num) \n",
    "\n",
    "    pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "    df_pca = pd.DataFrame(X_pca, columns = pca_columns)\n",
    "\n",
    "    df_modified = pd.concat(\n",
    "        [\n",
    "            df_pca, \n",
    "            pd.DataFrame(X_cat, columns = cat_col_names), \n",
    "            pd.Series(y, name = response_col_name)\n",
    "        ], \n",
    "        axis = 1)\n",
    "    \n",
    "    if logs:\n",
    "        print(f\"Number of components selected: {pca.n_components_}\")\n",
    "        print(\"Explained variance ratio for each component:\", pca.explained_variance_ratio_)\n",
    "        print(\"Cumulative explained variance:\", pca.explained_variance_ratio_.cumsum())\n",
    "        print(\"Final DataFrame with PCA applied to numeric columns:\")\n",
    "        print(df_modified.head())\n",
    "\n",
    "    return df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_variance(\n",
    "        df: pd.DataFrame, \n",
    "        num_col_names: list[str], \n",
    "        threshold: float = 0.1\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter numeric columns based on a variance threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        num_col_names (list[str]): List of numeric column names to evaluate.\n",
    "        threshold (float): The variance threshold for filtering columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with low variance numeric columns removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the variance for each numeric column\n",
    "    variances = df[num_col_names].var()\n",
    "\n",
    "    # Filter columns with variance greater than the threshold : default 0.1\n",
    "    high_variance_cols = variances[variances > threshold].index.tolist()\n",
    "\n",
    "    # Create a new DataFrame with only the selected columns\n",
    "    filtered_df = df[high_variance_cols]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_data(\n",
    "        df: pd.DataFrame, \n",
    "        response_col_name : str\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform undersampling to balance the classes in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        target_col (str): The name of the target column (class label).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A balanced DataFrame with equal samples from both classes.\n",
    "    \"\"\"\n",
    "    # Separate the majority and minority classes\n",
    "    majority_class = df[response_col_name].value_counts().idxmax()\n",
    "    minority_class = df[response_col_name].value_counts().idxmin()\n",
    "\n",
    "    # Split the DataFrame into majority and minority\n",
    "    df_majority = df[df[response_col_name] == majority_class]\n",
    "    df_minority = df[df[response_col_name] == minority_class]\n",
    "\n",
    "    # Randomly sample from the majority class\n",
    "    df_majority_sampled = df_majority.sample(n = len(df_minority), random_state = 42)\n",
    "\n",
    "    # Combine the minority class with the undersampled majority class\n",
    "    df_balanced = pd.concat([df_majority_sampled, df_minority])\n",
    "\n",
    "    # Shuffle the resulting DataFrame\n",
    "    df_balanced = df_balanced.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "\n",
    "    return df_balanced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
