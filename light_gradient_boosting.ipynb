{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import lightgbm as lgb\n",
    "import optuna \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import import_ipynb \n",
    "import Data_Processing as dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import cleaned and processed data frame from Data_processing file \n",
    "\n",
    "def main(): \n",
    "    client_df = dp.import_client()\n",
    "    invoice_df = dp.import_invoice()\n",
    "    client_df = dp.convert_date(client_df) # Convert date cols\n",
    "    invoice_df = dp.convert_date(invoice_df)\n",
    "    client_df = dp.drop_duplicates(client_df) # Drop duplicates rows\n",
    "    invoice_df = dp.drop_duplicates(invoice_df)\n",
    "    categorical_column_names = ['region', 'dis', 'id', 'catg', 'target']\n",
    "    client_df = dp.convert_to_categorical( # Convert categorical cols\n",
    "        client_df, cols = categorical_column_names\n",
    "        )\n",
    "    invoice_df = dp.aggregate_invoice(invoice_df) # Aggregate invoices\n",
    "    invoice_df = dp.manual_fix_names( # Fix column names manually\n",
    "        invoice_df, \n",
    "        new_col_names = [\n",
    "            'id', \n",
    "            'cons_level_1_sum', 'cons_level_1_mean', \n",
    "            'cons_level_1_max', 'cons_level_1_std',\n",
    "            'cons_level_2_sum', 'cons_level_2_mean', \n",
    "            'cons_level_2_max', 'cons_level_2_std',\n",
    "            'cons_level_3_sum', 'cons_level_3_mean', \n",
    "            'cons_level_3_max', 'cons_level_3_std',\n",
    "            'cons_level_4_sum', 'cons_level_4_mean', \n",
    "            'cons_level_4_max', 'cons_level_4_std',\n",
    "            'date_sum', 'date_mean', 'date_max', 'date_std',\n",
    "            'num_invoices'\n",
    "            ]\n",
    "        )\n",
    "    df = dp.merge(client_df = client_df, invoice_df = invoice_df) # Merge\n",
    "    df = dp.prep_dataframe( # Prep for PCA\n",
    "        df = df,\n",
    "        response_col_name = 'target',\n",
    "        cat_col_names = categorical_column_names\n",
    "    )\n",
    "    \n",
    "    df = dp.principal_component_analysis( # Do PCA\n",
    "        df = df,\n",
    "        response_col_name = 'target'\n",
    "    )\n",
    "    # Do Low Variance Filter\n",
    "    df = dp.filter_low_variance(df, response_col_name = 'target')\n",
    "    df = dp.balance_data( # Do balancing \n",
    "        df = df, \n",
    "        response_col_name = 'target', \n",
    "        prop_synthetic_data = 0.4 # Final proportion of synthetic data\n",
    "        ) \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if we should consider the categorical explanatory variables?\n",
    "def cat_df():    \n",
    "    client_df = dp.import_client()\n",
    "    invoice_df = dp.import_invoice()\n",
    "    client_df = dp.convert_date(client_df) # Convert date cols\n",
    "    invoice_df = dp.convert_date(invoice_df)\n",
    "    client_df = dp.drop_duplicates(client_df) # Drop duplicates rows\n",
    "    invoice_df = dp.drop_duplicates(invoice_df)\n",
    "    categorical_column_names = ['region', 'dis', 'id', 'catg', 'target']\n",
    "    client_df = dp.convert_to_categorical( # Convert categorical cols\n",
    "        client_df, cols = categorical_column_names\n",
    "        )\n",
    "    invoice_df = dp.aggregate_invoice(invoice_df) # Aggregate invoices\n",
    "    invoice_df = dp.manual_fix_names( # Fix column names manually\n",
    "        invoice_df, \n",
    "        new_col_names = [\n",
    "            'id', \n",
    "            'cons_level_1_sum', 'cons_level_1_mean', \n",
    "            'cons_level_1_max', 'cons_level_1_std',\n",
    "            'cons_level_2_sum', 'cons_level_2_mean', \n",
    "            'cons_level_2_max', 'cons_level_2_std',\n",
    "            'cons_level_3_sum', 'cons_level_3_mean', \n",
    "            'cons_level_3_max', 'cons_level_3_std',\n",
    "            'cons_level_4_sum', 'cons_level_4_mean', \n",
    "            'cons_level_4_max', 'cons_level_4_std',\n",
    "            'date_sum', 'date_mean', 'date_max', 'date_std',\n",
    "            'num_invoices'\n",
    "            ]\n",
    "        )\n",
    "    cate_df = dp.merge(client_df = client_df, invoice_df = invoice_df) # Merge\n",
    "    return cate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcdata = main()\n",
    "pcdata.head()\n",
    "\n",
    "#Convert categorical target to numeric \n",
    "pcdata['target'] = pcdata['target'].astype(int)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcdata[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns except for the last one \n",
    "X = pcdata.iloc[:,:-1]\n",
    "y = pcdata.iloc[:,-1]\n",
    "\n",
    "# Split data into train,validation and test set via a 80-10-10 split\n",
    "# First split into training + validation (90% of data) and test set (10% of data)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Second split into training (80% of total) and validation (10% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=1/9, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to search\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators',0, 100),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.15),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 128),\n",
    "        'min_split_gain': trial.suggest_loguniform(\"min_split_gain\",0.001,0.1),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 10)\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "    params, \n",
    "    train_data, \n",
    "    valid_sets=[valid_data], \n",
    "    num_boost_round=100, \n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    "      # Use a callback for early stopping\n",
    ")\n",
    "\n",
    "    # Predict and calculate the accuracy on the test set\n",
    "    y_val_pred_proba = model.predict(X_val)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:45,571] A new study created in memory with name: no-name-38770857-e07a-49f6-ad23-81a32f3b7a6d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:45,989] Trial 0 finished with value: 0.9151415689187555 and parameters: {'n_estimators': 55, 'learning_rate': 0.07107985528910561, 'num_leaves': 169, 'max_depth': 78, 'min_split_gain': 0.03632609723510134, 'feature_fraction': 0.916462725369, 'bagging_freq': 10}. Best is trial 0 with value: 0.9151415689187555.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.376783\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:46,372] Trial 1 finished with value: 0.8395840171368356 and parameters: {'n_estimators': 60, 'learning_rate': 0.06941823904386503, 'num_leaves': 171, 'max_depth': 30, 'min_split_gain': 0.004489759352571418, 'feature_fraction': 0.13386085340315546, 'bagging_freq': 2}. Best is trial 0 with value: 0.9151415689187555.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[60]\tvalid_0's binary_logloss: 0.517741\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:47,077] Trial 2 finished with value: 0.9392471539378445 and parameters: {'n_estimators': 74, 'learning_rate': 0.0747263751703812, 'num_leaves': 248, 'max_depth': 119, 'min_split_gain': 0.01104933255005825, 'feature_fraction': 0.4421182025740694, 'bagging_freq': 0}. Best is trial 2 with value: 0.9392471539378445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[74]\tvalid_0's binary_logloss: 0.331737\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:47,354] Trial 3 finished with value: 0.8702679358059668 and parameters: {'n_estimators': 34, 'learning_rate': 0.010333040840170404, 'num_leaves': 233, 'max_depth': 88, 'min_split_gain': 0.03733119868571864, 'feature_fraction': 0.22235367603898848, 'bagging_freq': 10}. Best is trial 2 with value: 0.9392471539378445.\n",
      "[I 2024-10-24 21:36:47,525] Trial 4 finished with value: 0.7896539810320107 and parameters: {'n_estimators': 87, 'learning_rate': 0.001621419844547359, 'num_leaves': 35, 'max_depth': 47, 'min_split_gain': 0.0012641256399731405, 'feature_fraction': 0.5655969417232354, 'bagging_freq': 3}. Best is trial 2 with value: 0.9392471539378445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.62575\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\tvalid_0's binary_logloss: 0.667324\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:47,923] Trial 5 finished with value: 0.9228773299704598 and parameters: {'n_estimators': 40, 'learning_rate': 0.10553298795214522, 'num_leaves': 386, 'max_depth': 17, 'min_split_gain': 0.05225878546281163, 'feature_fraction': 0.9858270688952944, 'bagging_freq': 2}. Best is trial 2 with value: 0.9392471539378445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.362212\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:48,870] Trial 6 finished with value: 0.9090918513655917 and parameters: {'n_estimators': 100, 'learning_rate': 0.013504218823901484, 'num_leaves': 261, 'max_depth': 59, 'min_split_gain': 0.029183024172381895, 'feature_fraction': 0.31647936240536656, 'bagging_freq': 1}. Best is trial 2 with value: 0.9392471539378445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.49422\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:49,693] Trial 7 finished with value: 0.911721111821307 and parameters: {'n_estimators': 86, 'learning_rate': 0.028989556020230327, 'num_leaves': 231, 'max_depth': 119, 'min_split_gain': 0.04058124468012041, 'feature_fraction': 0.9181968578093205, 'bagging_freq': 8}. Best is trial 2 with value: 0.9392471539378445.\n",
      "[I 2024-10-24 21:36:49,799] Trial 8 finished with value: 0.769062138303937 and parameters: {'n_estimators': 49, 'learning_rate': 0.0016061969174129187, 'num_leaves': 25, 'max_depth': 95, 'min_split_gain': 0.010571851149399571, 'feature_fraction': 0.8958792805893417, 'bagging_freq': 9}. Best is trial 2 with value: 0.9392471539378445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[86]\tvalid_0's binary_logloss: 0.399069\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.677965\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 21:36:50,193] Trial 9 finished with value: 0.8779207766856116 and parameters: {'n_estimators': 36, 'learning_rate': 0.001965147684503104, 'num_leaves': 446, 'max_depth': 65, 'min_split_gain': 0.025512769220791334, 'feature_fraction': 0.5586634955618639, 'bagging_freq': 4}. Best is trial 2 with value: 0.9392471539378445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.67073\n",
      "Best parameters found:  {'n_estimators': 74, 'learning_rate': 0.0747263751703812, 'num_leaves': 248, 'max_depth': 119, 'min_split_gain': 0.01104933255005825, 'feature_fraction': 0.4421182025740694, 'bagging_freq': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize\n",
    "  # We want to maximize accuracy\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters found: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.94)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use best parameters to train the model\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'binary'\n",
    "best_params['metric'] = 'binary_logloss'  \n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    valid_sets=[valid_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    ")\n",
    "\n",
    "# Predict and calculate the accuracy on the test set\n",
    "y_val_pred_proba = final_model.predict(X_val)\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "roc_auc.round(2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
