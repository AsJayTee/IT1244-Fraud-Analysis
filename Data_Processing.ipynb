{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Importing functions\n",
    "\n",
    "def import_client(filepath : str = \"Data/client.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to import client dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def import_invoice(filepath : str = \"Data/invoice.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to import invoice dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing functions\n",
    "\n",
    "def convert_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts date column to an integer representing \n",
    "    the number of days since the earliest date.\n",
    "    *Column name 'date' is fixed for both datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Find the earliest date\n",
    "    earliest_date = df['date'].min()\n",
    "    \n",
    "    # Calculate the number of days since the earliest date\n",
    "    df['date'] = (df['date'] - earliest_date).dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_duplicates(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prints the result of a duplicate check.\n",
    "    Drops duplicates if they exist.\n",
    "    \"\"\"\n",
    "    if df.duplicated().any(): # Duplicates check\n",
    "        print(\"Duplicates found! Cleaning them up...\")\n",
    "        df = df.drop_duplicates() # Drops duplicates\n",
    "        df = df.reset_index(drop = True) # Resets indexes\n",
    "    else:\n",
    "        print(\"No duplicates found!\")\n",
    "    return df\n",
    "\n",
    "def convert_to_categorical(\n",
    "        df : pd.DataFrame,\n",
    "        cols : list[str]\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts list of column names to categorical datatype.\n",
    "    \"\"\"\n",
    "    df[cols] = df[cols].astype('category')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Feature Engineering functions\n",
    "\n",
    "def aggregate_invoice(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate the invoice dataframe by id and generate features.\n",
    "    Calculates sum, mean, max, and std for each consommation_level and date\n",
    "    and counts number of invoices under each id.\n",
    "    \"\"\"\n",
    "    df = df.groupby('id').agg({ # Aggregate by id\n",
    "        # Calculate sum, mean, max, and std for each consm_level and date\n",
    "        'consommation_level_1': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_2': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_3': ['sum', 'mean', 'max', 'std'],\n",
    "        'consommation_level_4': ['sum', 'mean', 'max', 'std'],\n",
    "        'date': ['sum', 'mean', 'max', 'std'],\n",
    "        'counter_statue': 'count', # Count number of invoices\n",
    "    }).reset_index()\n",
    "    df.columns = [\n",
    "        # Join by _ if more than 2 parts to the column name exists\n",
    "        '_'.join(col).strip() if col[1] \n",
    "        else col[0] # Else use original name\n",
    "        for col in df.columns.values # For each column name value\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def manual_fix_names(\n",
    "        df : pd.DataFrame,\n",
    "        new_col_names : list[str]\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Manually sets the column names of a dataframe.\n",
    "    \"\"\"\n",
    "    df.columns = new_col_names\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Joining functions\n",
    "\n",
    "def merge(\n",
    "        client_df : pd.DataFrame,\n",
    "        invoice_df : pd.DataFrame,\n",
    "        merge_by : str = \"id\"\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges two dataframes.\n",
    "    Merges on 'id' column by default (for client and invoice).\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(\n",
    "        client_df, invoice_df, on = merge_by)\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensionality Reduction functions\n",
    "\n",
    "def prep_dataframe(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [],\n",
    "        scale : bool = True,\n",
    "        OHE : bool = True\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For scaling and one-hot encoding dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        scale (bool): Indicate if data should be scaled\n",
    "        OHE (bool): Indicate if categorical variables should undergo One-Hot Encoding\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns scaled/One-Hot Encoded\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.drop(columns = ['id']) \n",
    "    # Drop id column as it is not useful for predicting\n",
    "\n",
    "    cat_col_names.remove('id') \n",
    "    # Remove 'id' as it no longer exists in dataframe\n",
    "\n",
    "    if response_col_name in cat_col_names:\n",
    "        cat_col_names.remove(response_col_name)\n",
    "    # Exclude doing OHE on response column\n",
    "\n",
    "    # Identify numerical columns \n",
    "    # by excluding categorical and response columns\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in cat_col_names \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    if scale:\n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "    if OHE:\n",
    "        # One-hot encode categorical features\n",
    "        encoder = OneHotEncoder(sparse_output = False, drop = 'first')  \n",
    "        # drop = 'first' to avoid dummy variable trap\n",
    "        X_cat = encoder.fit_transform(X_cat)\n",
    "    \n",
    "    # Combine the scaled numerical and encoded categorical features\n",
    "    X_prep = pd.DataFrame(\n",
    "        data = np.hstack((X_num, X_cat)),  # Horizontal stack to combine arrays\n",
    "        columns = num_col_names + list(encoder.get_feature_names_out(cat_col_names))\n",
    "    )\n",
    "\n",
    "    # Create a new DataFrame including the response variable\n",
    "    prep_df = pd.DataFrame(X_prep, columns = X_prep.columns)\n",
    "    prep_df[response_col_name] = y\n",
    "\n",
    "    return prep_df\n",
    "\n",
    "def principal_component_analysis(\n",
    "        df : pd.DataFrame, \n",
    "        response_col_name : str,\n",
    "        cat_col_names : list[str] = [], \n",
    "        var : float = 0.95, \n",
    "        logs : bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For reducing the dimensions of a dataframe using PCA.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        response_col_name (str): Name of response column\n",
    "        cat_col_names (list[str]): List of categorical column names to evaluate.\n",
    "        var (float): Proportion of variance that should be preserved\n",
    "        logs (bool): Indicate if logs should be printed\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame that has undergone PCA\n",
    "    \"\"\"\n",
    "\n",
    "    if response_col_name in cat_col_names:\n",
    "        cat_col_names.remove(response_col_name)\n",
    "    # Exclude doing OHE on response column\n",
    "\n",
    "    # Identify numerical columns \n",
    "    # by excluding categorical and response columns\n",
    "    num_col_names = [\n",
    "        col for col in df.columns \n",
    "        if col not in cat_col_names \n",
    "        and col != response_col_name\n",
    "    ]\n",
    "\n",
    "    y = df[response_col_name].values # Response column\n",
    "    X_num = df[num_col_names].values # Numerical columns\n",
    "    X_cat = df[cat_col_names].values # Categorical columns\n",
    "\n",
    "    pca = PCA(n_components = var) \n",
    "    # Keep 'var' proportion of the variance : default 95%\n",
    "    X_pca = pca.fit_transform(X_num) \n",
    "\n",
    "    pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "    df_pca = pd.DataFrame(X_pca, columns = pca_columns)\n",
    "\n",
    "    df_modified = pd.concat(\n",
    "        [\n",
    "            df_pca, \n",
    "            pd.DataFrame(X_cat, columns = cat_col_names), \n",
    "            pd.Series(y, name = response_col_name)\n",
    "        ], \n",
    "        axis = 1)\n",
    "    \n",
    "    if logs:\n",
    "        print(f\"Number of components selected: {pca.n_components_}\")\n",
    "        print(\"Explained variance ratio for each component:\", pca.explained_variance_ratio_)\n",
    "        print(\"Cumulative explained variance:\", pca.explained_variance_ratio_.cumsum())\n",
    "        print(\"Final DataFrame with PCA applied to numeric columns:\")\n",
    "        print(df_modified.head())\n",
    "\n",
    "    return df_modified\n",
    "\n",
    "def filter_low_variance(\n",
    "        df: pd.DataFrame, \n",
    "        num_col_names: list[str], \n",
    "        threshold: float = 0.1\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter numeric columns based on a variance threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        num_col_names (list[str]): List of numeric column names to evaluate.\n",
    "        threshold (float): The variance threshold for filtering columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with low variance numeric columns removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the variance for each numeric column\n",
    "    variances = df[num_col_names].var()\n",
    "\n",
    "    # Filter columns with variance greater than the threshold : default 0.1\n",
    "    high_variance_cols = variances[variances > threshold].index.tolist()\n",
    "\n",
    "    # Create a new DataFrame with only the selected columns\n",
    "    filtered_df = df[high_variance_cols]\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define main workflow\n",
    "\n",
    "def main(): \n",
    "    client_df = import_client()\n",
    "    invoice_df = import_invoice()\n",
    "    client_df = convert_date(client_df) # Convert date cols\n",
    "    invoice_df = convert_date(invoice_df)\n",
    "    client_df = drop_duplicates(client_df) # Drop duplicates rows\n",
    "    invoice_df = drop_duplicates(invoice_df)\n",
    "    categorical_column_names = ['region', 'dis', 'id', 'catg', 'target']\n",
    "    client_df = convert_to_categorical( # Convert categorical cols\n",
    "        client_df, cols = categorical_column_names\n",
    "        )\n",
    "    invoice_df = aggregate_invoice(invoice_df) # Aggregate invoices\n",
    "    invoice_df = manual_fix_names( # Fix column names manually\n",
    "        invoice_df, \n",
    "        new_col_names = [\n",
    "            'id', \n",
    "            'cons_level_1_sum', 'cons_level_1_mean', \n",
    "            'cons_level_1_max', 'cons_level_1_std',\n",
    "            'cons_level_2_sum', 'cons_level_2_mean', \n",
    "            'cons_level_2_max', 'cons_level_2_std',\n",
    "            'cons_level_3_sum', 'cons_level_3_mean', \n",
    "            'cons_level_3_max', 'cons_level_3_std',\n",
    "            'cons_level_4_sum', 'cons_level_4_mean', \n",
    "            'cons_level_4_max', 'cons_level_4_std',\n",
    "            'date_sum', 'date_mean', 'date_max', 'date_std',\n",
    "            'num_invoices'\n",
    "            ]\n",
    "        )\n",
    "    df = merge(client_df = client_df, invoice_df = invoice_df)\n",
    "    df = prep_dataframe(\n",
    "        df = df,\n",
    "        response_col_name = 'target',\n",
    "        cat_col_names = categorical_column_names\n",
    "    )\n",
    "    print(df.columns.values)\n",
    "    df = principal_component_analysis(\n",
    "        df = df,\n",
    "        response_col_name = 'target'\n",
    "    )\n",
    "    print(df.columns.values)\n",
    "    print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siah Jin Thau\\AppData\\Local\\Temp\\ipykernel_4076\\3343288833.py:11: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['date'] = pd.to_datetime(df['date'])\n",
      "C:\\Users\\Siah Jin Thau\\AppData\\Local\\Temp\\ipykernel_4076\\3343288833.py:11: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['date'] = pd.to_datetime(df['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found!\n",
      "Duplicates found! Cleaning them up...\n",
      "['date' 'cons_level_1_sum' 'cons_level_1_mean' 'cons_level_1_max'\n",
      " 'cons_level_1_std' 'cons_level_2_sum' 'cons_level_2_mean'\n",
      " 'cons_level_2_max' 'cons_level_2_std' 'cons_level_3_sum'\n",
      " 'cons_level_3_mean' 'cons_level_3_max' 'cons_level_3_std'\n",
      " 'cons_level_4_sum' 'cons_level_4_mean' 'cons_level_4_max'\n",
      " 'cons_level_4_std' 'date_sum' 'date_mean' 'date_max' 'date_std'\n",
      " 'num_invoices' 'region_103' 'region_104' 'region_105' 'region_106'\n",
      " 'region_107' 'region_206' 'region_301' 'region_302' 'region_303'\n",
      " 'region_304' 'region_305' 'region_306' 'region_307' 'region_308'\n",
      " 'region_309' 'region_310' 'region_311' 'region_312' 'region_313'\n",
      " 'region_371' 'region_372' 'region_379' 'region_399' 'dis_62' 'dis_63'\n",
      " 'dis_69' 'catg_12' 'catg_51' 'target']\n",
      "['PC1' 'PC2' 'PC3' 'PC4' 'PC5' 'PC6' 'PC7' 'PC8' 'PC9' 'PC10' 'PC11'\n",
      " 'PC12' 'PC13' 'PC14' 'PC15' 'PC16' 'target']\n",
      "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "0 -0.156303  1.455693 -0.289557 -0.772065  0.427308 -0.713407  0.381298   \n",
      "1  0.184205  1.865908 -0.485605 -0.742034  0.395662 -0.369473 -0.078411   \n",
      "2 -0.009460  1.080616  0.426119  0.216862  1.281985 -0.362985  0.522426   \n",
      "3 -1.511789 -0.864854  0.245968 -0.411237  0.052633 -1.270572  0.218413   \n",
      "4  0.776524 -0.929508 -0.486357  1.165433 -1.434792  0.799974 -0.808864   \n",
      "\n",
      "        PC8       PC9      PC10      PC11      PC12      PC13      PC14  \\\n",
      "0 -0.260961  0.091013  0.154409  0.093105 -0.111642  0.133808 -0.058840   \n",
      "1  0.258837 -0.098360 -0.055175 -0.107078 -0.968826 -0.449584 -0.201366   \n",
      "2 -1.629008  1.020376 -0.377531 -0.097228  0.053216 -0.347132  0.112169   \n",
      "3 -0.048031 -0.276486  0.404534  0.517248 -0.448144 -0.285221 -0.006446   \n",
      "4 -0.599817 -0.128966 -0.757656 -0.082450  0.644964 -0.201014 -0.354725   \n",
      "\n",
      "       PC15      PC16 target  \n",
      "0 -0.068390  0.081431      0  \n",
      "1  0.072139  0.486190      0  \n",
      "2  0.130270  0.046122      0  \n",
      "3 -0.179386  0.047138      0  \n",
      "4 -0.212332 -0.167950      0  \n"
     ]
    }
   ],
   "source": [
    "## Run to execute main workflow\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
